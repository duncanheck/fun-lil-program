#!/usr/bin/env python3
"""
learn_calibration.py

Designed to run on *_results.xlsx files generated by engine.py.
Extracts predicted vs measured values for:
- Moisture content
- Outlet RH
- Outlet temperature
- D50
- Morphology

Computes multiplicative calibration factors (observed = factor * predicted) via least-squares.
Trains ML models for prediction of these values.
Persists calibration factors to calibration.json and adds/updates "Calibration" sheet in Excel.

Usage:
  python spraydrier/ml/learn_calibration.py
"""

import json
import math
import pickle
from pathlib import Path
import os
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, classification_report
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.feature_selection import SelectKBest, f_regression, f_classif
from sklearn.pipeline import Pipeline

def read_history(workbook_path: Path):
    if not workbook_path.exists():
        print(f"No workbook found at {workbook_path}")
        return None
    try:
        df = pd.read_excel(workbook_path)
        # Detect format: if 'trial_id' is a column, it's results format (trials as rows)
        if 'trial_id' in df.columns:
            print("Detected results format (trials as rows)")
            df = df.set_index('trial_id').T  # Transpose to params as rows, trials as columns for consistency
        else:
            df = df.set_index(df.columns[0]).T  # Assume params as rows
        print(f"Loaded workbook with {len(df.columns)} trials")
        # Debug: show all row names
        print("\nAvailable row names in file (first 30):")
        row_names = list(df.index)
        print(row_names[:30])
        if len(row_names) > 30:
            print("... (truncated)")
        return df
    except Exception as e:
        print(f"Error reading workbook: {e}")
        return None

def extract_pairs(df: pd.DataFrame, pred_names, meas_names):
    pairs = []
    found_matches = []

    for col in df.columns:
        pred = None
        pred_name_used = None
        for pred_name in pred_names:
            try:
                if pred_name in df.index:
                    pred = df.at[pred_name, col]
                    if not pd.isna(pred):
                        pred_name_used = pred_name
                        break
            except KeyError:
                continue
        
        meas = None
        meas_name_used = None
        for meas_name in meas_names:
            try:
                if meas_name in df.index:
                    meas = df.at[meas_name, col]
                    if not pd.isna(meas):
                        meas_name_used = meas_name
                        break
            except KeyError:
                continue

        if pred is not None and meas is not None:
            try:
                pred_val = float(pred)
                meas_val = float(meas)
                if meas_val > 1: meas_val /= 100.0
                if pred_val > 1: pred_val /= 100.0
                pairs.append((pred_val, meas_val))
                found_matches.append(f"Col {col}: pred='{pred_name_used}' ({pred_val:.4f}), meas='{meas_name_used}' ({meas_val:.4f})")
            except Exception as e:
                print(f"  Skipping col {col}: conversion error {e}")
                continue

    if pairs:
        print(f"  Found {len(pairs)} valid pairs:")
        for match in found_matches:
            print(f"    {match}")
    else:
        print("  No valid predicted/measured pairs found for this metric")
        print(f"  Searched predicted names: {pred_names}")
        print(f"  Searched measured names: {meas_names}")
        print("  Tip: Use *_results.xlsx from engine.py — it has predicted columns")

    return pairs

def fit_multiplicative(pairs):
    if len(pairs) < 2:
        print("  Too few pairs (<2) — using default factor 1.0")
        return 1.0
    xs = np.array([p[0] for p in pairs])
    ys = np.array([p[1] for p in pairs])
    denom = np.dot(xs, xs)
    if denom == 0:
        print("  Denominator zero — using default factor 1.0")
        return 1.0
    k = float(np.dot(xs, ys) / denom)
    print(f"  Fitted factor: {k:.6f} (from {len(pairs)} pairs)")
    return k

def extract_moisture_training_data(df: pd.DataFrame):
    training_data = []
    feature_columns = [
        'T1_C', 'T_inlet', 'Drying Gas Inlet (C)',
        'feed_g_min', 'Feed Rate (g/min)',
        '%Solids', 'solids_fraction', 'solids_frac',
        'm1_m3ph', 'Drying gas rate (m³/hr)',
        'atomization_pressure', 'Atomization Pressure',
        'moisture_content', 'ds_conc', 'nozzle_tip_d_mm', 'V_chamber_m3',
        'effective_pe_drug', 'integrated_pe'
    ]
    for col in df.columns:
        target = None
        for name in ['measured_total_moisture', 'Measured total moisture (%)', 'powder_residual_moisture']:
            try:
                target = df.at[name, col]
                if pd.notna(target):
                    break
            except KeyError:
                continue
        if target is None or pd.isna(target):
            continue
        try:
            target_val = float(target)
            if target_val > 1: target_val /= 100.0
            features = {}
            for f in feature_columns:
                try:
                    val = df.at[f, col]
                    if pd.notna(val):
                        features[f] = float(val)
                except (KeyError, ValueError):
                    continue
            if len(features) >= 4:
                training_data.append({'features': features, 'target': target_val})
        except Exception:
            continue
    return training_data

def train_moisture_model(training_data):
    if len(training_data) < 5:
        print(f"Insufficient data for moisture model: {len(training_data)} samples")
        return None, None, None

    all_features = sorted(set().union(*(d['features'].keys() for d in training_data)))
    print(f"Training moisture model with {len(training_data)} samples and {len(all_features)} features")

    X = []
    y = []
    for d in training_data:
        vec = [d['features'].get(f, np.nan) for f in all_features]
        X.append(vec)
        y.append(d['target'])
    X = np.array(X)
    y = np.array(y)

    imputer = SimpleImputer(strategy='mean')
    scaler = StandardScaler()
    selector = SelectKBest(score_func=f_regression, k='all')
    model = RandomForestRegressor(n_estimators=200, random_state=42)

    pipeline = Pipeline([
        ('imputer', imputer),
        ('scaler', scaler),
        ('selector', selector),
        ('model', model)
    ])

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    pipeline.fit(X_train, y_train)
    y_pred = pipeline.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    print(f"Moisture model - MSE: {mse:.6f}, R²: {r2:.3f}")

    return pipeline, all_features, imputer

def extract_outlet_temp_training_data(df: pd.DataFrame):
    training_data = []
    feature_columns = [
        'T1_C', 'T_inlet', 'Drying Gas Inlet (C)',
        'feed_g_min', 'Feed Rate (g/min)',
        '%Solids', 'solids_fraction',
        'm1_m3ph', 'gas_rate_m3_hr',
        'atomization_pressure', 'Atomization Pressure',
        'moisture_content', 'ds_conc', 'nozzle_tip_d_mm', 'V_chamber_m3',
        'effective_pe_drug', 'integrated_pe'
    ]
    for col in df.columns:
        target = None
        for name in ['T_outlet_C', 'measured_T_outlet_C', 'Measured Outlet Temperature (C)']:
            try:
                target = df.at[name, col]
                if pd.notna(target):
                    break
            except KeyError:
                continue
        if target is None or pd.isna(target):
            continue
        try:
            target_val = float(target)
            features = {}
            for f in feature_columns:
                try:
                    val = df.at[f, col]
                    if pd.notna(val):
                        features[f] = float(val)
                except (KeyError, ValueError):
                    continue
            if len(features) >= 4:
                training_data.append({'features': features, 'target': target_val})
        except Exception:
            continue
    return training_data

def train_outlet_temp_model(training_data):
    if len(training_data) < 5:
        print(f"Insufficient data for outlet temp model: {len(training_data)} samples")
        return None, None, None

    all_features = sorted(set().union(*(d['features'].keys() for d in training_data)))
    print(f"Training outlet temp model with {len(training_data)} samples and {len(all_features)} features")

    X = []
    y = []
    for d in training_data:
        vec = [d['features'].get(f, np.nan) for f in all_features]
        X.append(vec)
        y.append(d['target'])
    X = np.array(X)
    y = np.array(y)

    imputer = SimpleImputer(strategy='mean')
    scaler = StandardScaler()
    selector = SelectKBest(score_func=f_regression, k='all')
    model = GradientBoostingRegressor(n_estimators=100, random_state=42)

    pipeline = Pipeline([
        ('imputer', imputer),
        ('scaler', scaler),
        ('selector', selector),
        ('model', model)
    ])

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    pipeline.fit(X_train, y_train)
    y_pred = pipeline.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    print(f"Outlet temp model - MSE: {mse:.2f}, R²: {r2:.3f}")

    return pipeline, all_features, imputer

def extract_morphology_training_data(df: pd.DataFrame):
    training_data = []
    feature_columns = [
        'T1_C', 'feed_g_min', '%Solids', 'm1_m3ph', 'atomization_pressure',
        'ds_conc', 'moni_conc', 'stab_A_conc', 'buffer_conc',
        'effective_pe_drug', 'integrated_pe', 'shell_formation_time',
        'final_tg_c', 'nozzle_tip_d_mm', 'V_chamber_m3', 'pH'
    ]
    for col in df.columns:
        target = None
        for name in ['Morphology', 'morphology', 'Particle Morphology', 'predicted_morphology']:
            try:
                target = df.at[name, col]
                if pd.notna(target) and str(target).strip():
                    break
            except KeyError:
                continue
        if target is None:
            continue
        features = {}
        for f in feature_columns:
            try:
                val = df.at[f, col]
                if pd.notna(val):
                    features[f] = float(val)
            except (KeyError, ValueError):
                continue
        if len(features) >= 5:
            training_data.append({'features': features, 'target': str(target).strip()})
    return training_data

def train_morphology_model(training_data):
    if len(training_data) < 10:
        print(f"Insufficient data for morphology model: {len(training_data)} samples")
        return None, None, None, None

    all_features = sorted(set().union(*(d['features'].keys() for d in training_data)))
    print(f"Training morphology model with {len(training_data)} samples and {len(all_features)} features")

    X = []
    y = []
    for d in training_data:
        vec = [d['features'].get(f, np.nan) for f in all_features]
        X.append(vec)
        y.append(d['target'])
    X = np.array(X)
    y = np.array(y)

    label_encoder = LabelEncoder()
    y_encoded = label_encoder.fit_transform(y)

    print(f"Morphology classes found: {label_encoder.classes_}")
    print(f"Class distribution: {np.bincount(y_encoded)}")

    imputer = SimpleImputer(strategy='mean')
    scaler = StandardScaler()
    selector = SelectKBest(score_func=f_classif, k='all')
    model = RandomForestClassifier(n_estimators=200, class_weight='balanced', random_state=42)

    pipeline = Pipeline([
        ('imputer', imputer),
        ('scaler', scaler),
        ('selector', selector),
        ('model', model)
    ])

    class_counts = np.bincount(y_encoded)
    min_class_count = np.min(class_counts[class_counts > 0]) if np.any(class_counts > 0) else 0
    use_stratify = min_class_count >= 2

    print(f"Using stratification: {use_stratify} (min class count = {min_class_count})")

    X_train, X_test, y_train, y_test = train_test_split(
        X, y_encoded,
        test_size=0.2,
        random_state=42,
        stratify=y_encoded if use_stratify else None
    )

    print(f"Train set size: {len(X_train)}, Test set size: {len(X_test)}")
    if len(X_test) == 0:
        print("Warning: Test set empty — training on full data only")
        pipeline.fit(X, y_encoded)
        return pipeline, all_features, label_encoder, imputer

    pipeline.fit(X_train, y_train)
    y_pred = pipeline.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    print(f"Morphology model - Accuracy: {acc:.3f}")

    try:
        print("\nClassification Report (all trained classes):")
        print(classification_report(
            y_test, y_pred,
            labels=range(len(label_encoder.classes_)),
            target_names=label_encoder.classes_,
            zero_division=0
        ))
    except Exception as e:
        print(f"Classification report failed: {e}")
        print("Test set class distribution:", np.bincount(y_test))
        print("Predicted class distribution:", np.bincount(y_pred))

    return pipeline, all_features, label_encoder, imputer

def write_calibration_json(calib_factors, path: Path):
    with open(path, 'w') as f:
        json.dump(calib_factors, f, indent=2)
    print(f"Wrote calibration to {path}")

def write_calibration_sheet(workbook_path: Path, calib_factors):
    try:
        with pd.ExcelWriter(workbook_path, mode='a', if_sheet_exists='replace', engine='openpyxl') as writer:
            pd.DataFrame([calib_factors]).to_excel(writer, sheet_name='Calibration', index=False)
        print(f"Updated 'Calibration' sheet in {workbook_path}")
    except Exception as e:
        print(f"Error updating workbook: {e}")

def main():
    print("Learn Calibration Factors & Train Models")
    print("=" * 60)

    # Prefer results file for calibration
    default_file = "snapshot_mab_input (1)_results.xlsx"
    workbook_input = input(f"Enter path to *_results.xlsx file (default: {default_file}): ").strip() or default_file
    workbook_path = Path(workbook_input)
    if not workbook_path.is_absolute():
        workbook_path = Path.cwd() / workbook_input

    if not workbook_path.exists():
        print(f"File not found: {workbook_path}")
        return

    df = read_history(workbook_path)
    if df is None:
        return

    calib_factors = {}

    # Moisture (updated search lists for results files)
    moisture_pairs = extract_pairs(df,
        pred_names=[
            'moisture_predicted',
            'Predicted powder moisture content (%)',
            'calc_moisture_pct',
            'predicted_moisture_content',
            'moisture_calc',
            'Powder moisture (calc)'
        ],
        meas_names=[
            'Measured total moisture (%)',
            'measured_total_moisture',
            'Measured moisture (%)',
            'total_moisture_measured',
            'Measured Total Moisture (%)',
            'moisture_content'
        ])
    k_moisture = fit_multiplicative(moisture_pairs)
    calib_factors['moisture_factor'] = k_moisture

    # RH
    rh_pairs = extract_pairs(df,
        pred_names=[
            'RH_out',
            'est_RH_pct',
            'est_RH_pct_realistic',
            'predicted_RH_outlet',
            'RH_out_calc',
            'Estimated RH_outlet'
        ],
        meas_names=[
            'measured_RH_out',
            'Outlet Relative Humidity',
            'RH_out_measured',
            'measured_RH',
            'Measured RH_out'
        ])
    k_rh = fit_multiplicative(rh_pairs)
    calib_factors['rh_factor'] = k_rh

    # Outlet temperature
    outlet_pairs = extract_pairs(df,
        pred_names=[
            'T_outlet_C',
            'predicted_outlet_temp_C',
            'est_T_outlet_C',
            'Predicted Outlet Temperature (C)'
        ],
        meas_names=[
            'measured_T_outlet_C',
            'T_outlet_measured',
            'Measured Outlet Temperature (C)',
            'Outlet (C)'
        ])
    k_t_outlet = fit_multiplicative(outlet_pairs)
    calib_factors['outlet_temp_factor'] = k_t_outlet

    # D50
    d50_pairs = extract_pairs(df,
        pred_names=[
            'D50_calc',
            'D50_calc_moni',
            'predicted_D50',
            'Calculated D50'
        ],
        meas_names=[
            'D50_actual',
            'Measured D50 (um)',
            'D50_measured',
            'Actual D50'
        ])
    k_d50 = fit_multiplicative(d50_pairs)
    calib_factors['d50_factor'] = k_d50

    # Train ML models
    moisture_data = extract_moisture_training_data(df)
    moisture_model, moisture_features, moisture_imputer = train_moisture_model(moisture_data)

    outlet_data = extract_outlet_temp_training_data(df)
    outlet_model, outlet_features, outlet_imputer = train_outlet_temp_model(outlet_data)

    morph_data = extract_morphology_training_data(df)
    morph_model, morph_features, morph_encoder, morph_imputer = train_morphology_model(morph_data)

    # Create ml folder if missing
    ml_dir = Path("ml")
    ml_dir.mkdir(exist_ok=True)

    # Save calibration
    calib_path = Path("calibration.json")
    write_calibration_json(calib_factors, calib_path)

    # Save models
    if moisture_model:
        with open(ml_dir / "moisture_model.pkl", 'wb') as f:
            pickle.dump({'model': moisture_model, 'features': moisture_features, 'imputer': moisture_imputer}, f)
        print("Saved moisture model to ml/moisture_model.pkl")

    if outlet_model:
        with open(ml_dir / "outlet_temp_model.pkl", 'wb') as f:
            pickle.dump({'model': outlet_model, 'features': outlet_features, 'imputer': outlet_imputer}, f)
        print("Saved outlet temp model to ml/outlet_temp_model.pkl")

    if morph_model:
        with open(ml_dir / "morphology_model.pkl", 'wb') as f:
            pickle.dump({'model': morph_model, 'features': morph_features, 'encoder': morph_encoder, 'imputer': morph_imputer}, f)
        print("Saved morphology model to ml/morphology_model.pkl")

    # Update Excel with Calibration sheet
    write_calibration_sheet(workbook_path, calib_factors)

    print("\nCalibration & training complete.")

if __name__ == '__main__':
    main()